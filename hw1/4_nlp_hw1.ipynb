{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b94de2",
   "metadata": {},
   "source": [
    "# Домашнее задание 1. Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bbe882",
   "metadata": {},
   "source": [
    "### 1. Подготовить мини-корпус (не меньше 4 текстов, примерный общий объём - 3-5 тысяч токенов) с разметкой ключевых слов. Предполагается, что вы найдете источник текстов, в котором уже выделены ключевые слова. Укажите источник корпуса и опишите, в каком виде там были представлены ключевые слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103cfde",
   "metadata": {},
   "source": [
    "Я взяла тексты с сайта CNN. У них в HTML коде страницы для многих новостей вшиты ключевые слова под тегом `meta name=\"keywords\n",
    "content=\"...\"`, например, \n",
    "```\n",
    "(угловая скобка) meta name=\"keywords\" content=\"arts and entertainment, basketball, celebrity and pop culture, companies, internet and www, luka doncic, nba, social media, sports and recreation, sports figures, sports organizations and teams, technology, twitter\" (угловая скобка)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860829a",
   "metadata": {},
   "source": [
    "Так как эти ключевые слова есть не для всех текстов, а сами тексты могут быть разной длины (это может быть и opinion piece длиной 3000 слов), то я отбирала тексты вручную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6412d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA SUPERSTAR LUKA DONCIC SAYS HE PREFERS CHESS TO TWITTER\n",
      "Much of the world seems to be talking about Elon Musk’s takeover of Twitter and his decision to charge $8 a month for a blue tick, but Luka Doncic appears to have more cerebral pursuits on his mind when he switches on his phone.\n",
      "After Musk’s $44 billion acquisition of Twitter was explained to the 23-year-old NBA star by a reporter at a media conference on Tuesday, all Doncic could respond with was a shrug of his shoulders and a simple reply.\n",
      "“I just play chess on my phone,” said the Dallas Mavericks star.\n",
      "The three-time All-NBA first teamer’s response prompted a follow-up question asking if the computer usually wins or if the basketball player does when he plays chess.\n",
      "His answer was so typically Doncic: “I play online. But I win, mostly.”\n",
      "It’s not the first time he’s shown his love for chess.\n",
      "After his game-winning performance against the Boston Celtics in February 2021, Doncic was asked how he chooses how to dissect a defense.\n",
      "“It’s like playing chess: you’ve got to take your time and see the moves,” he told TNT after his 31-point performance.\n",
      "Last week, Twitter said it would re-introduce a grey “Official” badge for select accounts to help confirm their identities.\n",
      "The decision came after Twitter was forced to fend off a wave of verified-account impostors, including some posing as former US President Donald Trump, Japanese gaming company Nintendo, and pharmaceutical company Eli Lilly, among others.\n",
      "These accounts were the result of Musk’s decision to offer a blue check mark to any account holder willing to pay $8 a month, no questions asked, as he races to find new ways to make money from the platform.\n",
      "Not that Doncic is averse to updating his Twitter feed.\n",
      "A quick scroll through Doncic’s blue tick verified Twitter account sees him celebrating the first basketball court built in Europe by an NBA team – his Mavericks team built a court in his homeland of Slovenia.\n",
      "And also showing his support for French football star Karim Benzema – who plays for Real Madrid which is where Doncic began is basketball career – following his Ballon D’Or victory in October.\n",
      "The 26-year-old looks to be devoted sneakerhead, as he thanks on Twitter many of the shoemakers and sponsors he’s had in recent weeks.\n"
     ]
    }
   ],
   "source": [
    "corpus = open('cnn_corpus.txt', encoding='UTF-8').read()\n",
    "    \n",
    "corpus = corpus.split('\\n—\\n')\n",
    "print(corpus[1]) #пример текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d8e129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total texts 10\n",
      "Total words 5153\n",
      "Text lengths [246, 393, 751, 395, 338, 405, 398, 258, 790, 1179]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total texts\", len(corpus))\n",
    "print(\"Total words\", sum(len(i.split()) for i in corpus))\n",
    "print(\"Text lengths\", list(len(i.split()) for i in corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58bb9b",
   "metadata": {},
   "source": [
    "### 2. Разметить ключевые слова самостоятельно. Оценить пересечение с имеющейся разметкой. Составить эталон разметки (например, пересечение или объединение вашей разметки и исходной)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84535cb2",
   "metadata": {},
   "source": [
    "Так выглядят ключевые слова: сначала -- оригинальные CNN-овские, потом -- мои ручные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27263f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asia, continents and regions, india, south asia, aviation and aerospace industry, business and industry sectors, business, economy and trade, space and astronomy, space industry, spacecraft and satellites, space exploration, space launches \n",
      "\n",
      " india, rocket, skyroot, launch, vikram \n",
      "---\n",
      "arts and entertainment, basketball, celebrity and pop culture, companies, internet and www, luka doncic, nba, social media, sports and recreation, sports figures, sports organizations and teams, technology, twitter \n",
      "\n",
      " nba, doncic, musk, twitter, chess, blue, verified \n",
      "---\n",
      "business and industry sectors, business, economy and trade, government and public administration, government organizations - us, labor and employment, labor disputes and negotiations, labor relations, labor strikes, labor unions, politics, rail transportation, railway, transportation and warehousing, transportation infrastructure, us congress, companies, csx corp, freight railway, freight transportation, compensation and benefits, employee leave \n",
      "\n",
      " freight, railroads, strike, union, labor, industry, deal, jefferies, sick days, sick leave, reject, management \n",
      "---\n",
      "agriculture departments, continents and regions, government and public administration, government bodies and offices, government departments and authorities, government organizations - us, north america, the americas, united states, us department of health and human services, us federal departments and agencies, us food and drug administration, air pollution, business and industry sectors, business, economy and trade, climate change, energy and environment, energy and utilities, environment and natural resources, greenhouse gases, pollution, food and drink \n",
      "\n",
      " fda, meat, lab-grown meat, upside foods, cultivated meat, cultivated chicken, cultured meat, cultured chicken, agriculture, food \n",
      "---\n",
      "business and industry sectors, business, economy and trade, electronic commerce, online and home shopping, retail and wholesale trade, companies, live nation entertainment, ticketmaster entertainment inc, celebrities, taylor swift \n",
      "\n",
      " ticketmaster, swift, eras tour, cancel, sales, tickets, millions, demand, unprecedented \n",
      "---\n",
      "health and medical, labor and employment, mental health, workers and professionals, civil disobedience, coups and attempted coups, domestic terrorism, government and public administration, government organizations - us, international relations and national security, national security, politics, protests and demonstrations, riots, societal issues, society, terrorism, terrorism and counter-terrorism, unrest, conflicts and war, us capitol insurrection, us congress, violence in society \n",
      "\n",
      " mental health, employee, employers, conversation, workforce, concern \n",
      "---\n",
      "australia, continents and regions, novak djokovic, oceania, sports figures, australian open, communicable disease control, coronavirus, diseases and disorders, grand slam tournaments, health and medical, immigration, immigration, citizenship and displacement, infectious diseases, international relations and national security, life forms, microscopic life, professional tennis, public health, respiratory diseases, sports and recreation, sports events, tennis, tennis events, vaccination and immunization, viruses \n",
      "\n",
      " australia, djokovic, tennis, australian open, grand slam, deported, visa, ban \n",
      "---\n",
      "business and industry sectors, business, economy and trade, company activities and management, energy and utilities, environment and natural resources, government and public administration, government support of business, landforms and ecosystems, oceans, oil and gas industry, special economic zones, africa, continents and regions, nigeria, western africa \n",
      "\n",
      " lagos, nigeria, lekki, port, ship, africa, economy, oil, refinery \n",
      "---\n",
      "business, economy and trade, continents and regions, economy and economic indicators, europe, government and public administration, jeremy hunt, northern europe, political figures - intl, political platforms and issues, politics, public finance, tax cuts, tax policy, taxes and taxation, united kingdom, economic conditions, economic decline, recession, economic policy, monetary policy \n",
      "\n",
      " united kingdom, uk, recession, inflation, minister, hunt, tax, truss, economy, government, debt, borrowing, decision, environment, selfin, public spending \n",
      "---\n",
      "cristiano ronaldo, football (soccer), manchester united fc, premier league, seasons of the year, soccer events, sports and recreation, sports events, sports figures, sports organizations and teams, summer (season), uefa champions league\n",
      " \n",
      "\n",
      " ronaldo, manchester united, manchester, united, morgan, club, ten hag, champions league, coach, team, season, behavior, interview, player\n",
      " \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "a = open('cnn_own_keywords.txt').read().split('\\n\\n') \n",
    "m = open('cnn_manual_keywords.txt').read().split('\\n\\n')\n",
    "assert len(a) == len(m) == 10\n",
    "for i in range(len(corpus)):\n",
    "    print(a[i], '\\n\\n', m[i], '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e17e0",
   "metadata": {},
   "source": [
    "Понятно, что у нас разные стратегии разметки ключевых слов -- во-первых, у них в начале идут общие темы и регионы, которых касается статья, даже если их названий в самой статье нет, во-вторых, у них многие из этих тем пересекаются (soccer events, sports events, sports and recreation как теги одной статьи и др.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44db738c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'india'}\n",
      "{'twitter', 'nba'}\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "{'mental health'}\n",
      "{'australian open', 'tennis', 'australia'}\n",
      "{'nigeria', 'africa'}\n",
      "{'united kingdom', 'recession'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print(set((j.strip() for j in a[i].split(','))).intersection(set((j.strip() for j in m[i].split(',')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62489094",
   "metadata": {},
   "source": [
    "Пересечение очень маленькое, но надо заметить, что есть эквивалентные теги, записанные по-разному: ticketmaster и ticketmaster entertainment inc, manchester united fc и manchester united, doncic и luka doncic -- попробуем добавлять все совместные подстроки в ключевые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7e5c67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'india', 'launch'}\n",
      "{'twitter', 'doncic', 'nba'}\n",
      "{'labor', 'strike', 'freight', 'union', 'industry'}\n",
      "{'agriculture', 'food'}\n",
      "{'ticketmaster', 'swift'}\n",
      "{'mental health'}\n",
      "{'australian open', 'grand slam', 'australia', 'djokovic', 'tennis'}\n",
      "{'nigeria', 'africa', 'oil', 'economy', 'port'}\n",
      "{'hunt', 'tax', 'recession', 'united kingdom', 'economy', 'government'}\n",
      "{'ronaldo', 'team', 'champions league', 'united', 'manchester', 'season', 'manchester united'}\n"
     ]
    }
   ],
   "source": [
    "gold_keywords = []\n",
    "for i in range(len(corpus)):\n",
    "    own = {j.strip() for j in a[i].split(',')}\n",
    "    mine = {j.strip() for j in m[i].split(',')}\n",
    "    common = set()\n",
    "    for tag in own:\n",
    "        if tag in m[i]:\n",
    "            common.add(tag)\n",
    "    for tag in mine:\n",
    "        if tag in a[i]:\n",
    "            common.add(tag)\n",
    "            \n",
    "    print(common)\n",
    "    \n",
    "    gold_keywords += [list(common)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b295d93",
   "metadata": {},
   "source": [
    "Наконец, есть случаи с названиями организаций -- у меня \"fda\", у них \"us food and drug administration\", но так как в тексте она называется именно FDA, то оставим там. Кроме того, я добавлю названия мест, организаций, событий, о которых идёт речь в статьях, которых нет в тегах CNN (как в статье с открытием нового порта в Нигерии надо добавить название самого порта -- Lekki -- так как оно часто встречается). Видимо, у них это не включается из-за слишком узкого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "917273cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'india', 'launch', 'skyroot', 'vikram'},\n",
       " {'doncic', 'nba', 'twitter'},\n",
       " {'freight', 'industry', 'labor', 'strike', 'union'},\n",
       " {'agriculture', 'fda', 'food', 'upside foods'},\n",
       " {'eras tour', 'swift', 'ticketmaster'},\n",
       " {'mental health'},\n",
       " {'australia', 'australian open', 'djokovic', 'grand slam', 'tennis'},\n",
       " {'africa', 'economy', 'lagos', 'lekki', 'nigeria', 'oil', 'port'},\n",
       " {'economy',\n",
       "  'government',\n",
       "  'hunt',\n",
       "  'jefferies',\n",
       "  'recession',\n",
       "  'tax',\n",
       "  'united kingdom'},\n",
       " {'champions league',\n",
       "  'manchester',\n",
       "  'manchester united',\n",
       "  'ronaldo',\n",
       "  'season',\n",
       "  'team',\n",
       "  'ten hag',\n",
       "  'united'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add = [\n",
    "    ['vikram', 'skyroot'], \n",
    "    [], \n",
    "    [], \n",
    "    ['fda', 'upside foods'], \n",
    "    ['eras tour'], \n",
    "    [], \n",
    "    [], \n",
    "    ['lekki', 'lagos'], \n",
    "    ['jefferies'], \n",
    "    ['ten hag']]\n",
    "\n",
    "for i in range(len(gold_keywords)):\n",
    "    gold_keywords[i] += to_add[i]\n",
    "    \n",
    "gold_keywords = list(set(i) for i in gold_keywords)\n",
    "gold_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a222a",
   "metadata": {},
   "source": [
    "### 3. Применить к этому корпусу 3 метода извлечения ключевых слов на выбор "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d948feb",
   "metadata": {},
   "source": [
    "Сначала применим предобработку к тексту: уберём стоп-слова и лемматизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f15ee829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katya\\AppData\\Roaming\\Python\\Python38\\site-packages\\spacy\\util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.2.0) was trained with spaCy v3.2 and may not be 100% compatible with the current version (3.3.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61ad30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_texts = [' '.join(list([token.lemma_.lower() for token in nlp(i) if token.lower_ not in stopwords])) for i in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c9df451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india private rocket vikram - s launched space \n",
      " indian startup skyroot aerospace launch country privately develop rocket , vikram - s , space friday support indian space research organisation ( isro ) . \n",
      " , country space industry dominate state - run isro , skyroot aerospace open sector private company . \n",
      " \" rocket launch help test validate majority technology vikram series orbital class space launch vehicle , include sub - system technology test pre - lift post - lift phase launch , \" accord skyroot aerospace . \n",
      " rocket , weigh 546 kilogram ( 1203 pound ) , launch sriharikota spaceport reach altitude 89.5 kilometer ( 55.6 mile ) . \n",
      " mission symbolize india private rocket launch \" potential new india , \" say pawan kumar chandana , co - founder skyroot aerospace launch . \n",
      " skyroot aerospace launch 2018 base southern tech hub hyderabad . found isro engineer raise $ 68 million funding , accord datum firm tracxn . \n",
      " year , startup enter agreement isro use indian space agency expertise access facility . \n",
      " space race private sector heat world . year , billionaire jeff bezos richard branson take supersonic joy ride edge space .\n"
     ]
    }
   ],
   "source": [
    "print(normalized_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c4d53",
   "metadata": {},
   "source": [
    "#### 1. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96241568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d2b0ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = tfidf.fit_transform(normalized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e96f52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "feat = tfidf.get_feature_names_out()\n",
    "tfidf_kw = []\n",
    "for i in range(len(corpus)):\n",
    "    doc = tfidf_data[i]\n",
    "    df = pd.DataFrame(doc.T.todense(), index=feat, columns=['weights'])\n",
    "    df = df.sort_values(by=['weights'], ascending=False)[:8] # в самом длинном списке ключевых слов 8 слов\n",
    "    tfidf_kw += [df.index.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a920477a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common {'skyroot', 'launch'}\n",
      "keywords not found {'vikram', 'india'}\n",
      "extra keywords {'skyroot aerospace', 'rocket', 'isro', 'space', 'aerospace', 'private'}\n",
      "TEXT 2\n",
      "keywords in common {'twitter', 'doncic', 'nba'}\n",
      "keywords not found set()\n",
      "extra keywords {'chess', 'star', 'musk', 'account', 'play'}\n",
      "TEXT 3\n",
      "keywords in common {'strike', 'union'}\n",
      "keywords not found {'freight', 'labor', 'industry'}\n",
      "extra keywords {'railroad', 'agreement', 'sick', 'member', 'reach', 'reject'}\n",
      "TEXT 4\n",
      "keywords in common {'food', 'fda'}\n",
      "keywords not found {'agriculture', 'upside foods'}\n",
      "extra keywords {'foods', 'cultivate', 'upside', 'meat', 'chicken', 'animal'}\n",
      "TEXT 5\n",
      "keywords in common {'ticketmaster', 'swift'}\n",
      "keywords not found {'eras tour'}\n",
      "extra keywords {'fan', 'tour', 'ticket', 'buy', 'ticketmaster say', 'demand'}\n",
      "TEXT 6\n",
      "keywords in common {'mental health'}\n",
      "keywords not found set()\n",
      "extra keywords {'employee', 'health', 'issue', 'kuhn', 'employer', 'mental', 'employee mental'}\n",
      "TEXT 7\n",
      "keywords in common {'australian open', 'australia', 'djokovic'}\n",
      "keywords not found {'tennis', 'grand slam'}\n",
      "extra keywords {'open', 'news', 'australian', 'ban', 'overturn'}\n",
      "TEXT 8\n",
      "keywords in common {'lekki', 'nigeria', 'port'}\n",
      "keywords not found {'oil', 'economy', 'lagos', 'africa'}\n",
      "extra keywords {'project', 'refinery', 'complete', 'construction', 'trade'}\n",
      "TEXT 9\n",
      "keywords in common {'government', 'hunt', 'united kingdom', 'recession'}\n",
      "keywords not found {'tax', 'economy', 'jefferies'}\n",
      "extra keywords {'kingdom', 'public', 'united', 'say'}\n",
      "TEXT 10\n",
      "keywords in common {'ronaldo', 'manchester united', 'united', 'manchester'}\n",
      "keywords not found {'team', 'ten hag', 'season', 'champions league'}\n",
      "extra keywords {'club', 'player', 'say', 'interview'}\n"
     ]
    }
   ],
   "source": [
    "tfidf_tp, tfidf_fn, tfidf_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    tfidf_tp_i = set(tfidf_kw[i]).intersection(gold_keywords[i])\n",
    "    tfidf_tp.append(tfidf_tp_i)\n",
    "    tfidf_fn_i = gold_keywords[i].difference(set(tfidf_kw[i]))\n",
    "    tfidf_fn.append(tfidf_fn_i)\n",
    "    tfidf_fp_i = set(tfidf_kw[i]).difference(gold_keywords[i])\n",
    "    tfidf_fp.append(tfidf_fp_i)\n",
    "    print('keywords in common', tfidf_tp_i)\n",
    "    print('keywords not found', tfidf_fn_i)\n",
    "    print('extra keywords', tfidf_fp_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5d9f8",
   "metadata": {},
   "source": [
    "#### 2. RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b1361c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "rake = RAKE.Rake(list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4ef4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_kw_ = []\n",
    "for i in range(len(corpus)):\n",
    "    rake_kw_ += [rake.run(normalized_texts[i], maxWords = 2, minFrequency=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "526fb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rake_kw = []\n",
    "for rkw in rake_kw_:\n",
    "    if rkw:\n",
    "        rake_kw += [[i[0] for i in rkw]]\n",
    "    else:\n",
    "        rake_kw += [[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9589681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['year'],\n",
       " ['year', 'time'],\n",
       " ['jefferies', 'union'],\n",
       " ['lab', 'fda'],\n",
       " ['ticketmaster'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['difficult decision', 'hunt', 'selfin', '000'],\n",
       " ['club', 'respect', 'ronaldo', 'year', 'interview']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rake_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2f23643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common set()\n",
      "keywords not found {'vikram', 'skyroot', 'india', 'launch'}\n",
      "extra keywords {'year'}\n",
      "TEXT 2\n",
      "keywords in common set()\n",
      "keywords not found {'twitter', 'doncic', 'nba'}\n",
      "extra keywords {'year', 'time'}\n",
      "TEXT 3\n",
      "keywords in common {'union'}\n",
      "keywords not found {'freight', 'strike', 'labor', 'industry'}\n",
      "extra keywords {'jefferies'}\n",
      "TEXT 4\n",
      "keywords in common {'fda'}\n",
      "keywords not found {'agriculture', 'food', 'upside foods'}\n",
      "extra keywords {'lab'}\n",
      "TEXT 5\n",
      "keywords in common {'ticketmaster'}\n",
      "keywords not found {'eras tour', 'swift'}\n",
      "extra keywords set()\n",
      "TEXT 6\n",
      "keywords in common set()\n",
      "keywords not found {'mental health'}\n",
      "extra keywords set()\n",
      "TEXT 7\n",
      "keywords in common set()\n",
      "keywords not found {'australian open', 'australia', 'djokovic', 'tennis', 'grand slam'}\n",
      "extra keywords set()\n",
      "TEXT 8\n",
      "keywords in common set()\n",
      "keywords not found {'oil', 'nigeria', 'economy', 'lagos', 'port', 'lekki', 'africa'}\n",
      "extra keywords set()\n",
      "TEXT 9\n",
      "keywords in common {'hunt'}\n",
      "keywords not found {'tax', 'recession', 'economy', 'united kingdom', 'government', 'jefferies'}\n",
      "extra keywords {'difficult decision', 'selfin', '000'}\n",
      "TEXT 10\n",
      "keywords in common {'ronaldo'}\n",
      "keywords not found {'team', 'champions league', 'ten hag', 'united', 'manchester', 'season', 'manchester united'}\n",
      "extra keywords {'year', 'club', 'respect', 'interview'}\n"
     ]
    }
   ],
   "source": [
    "rake_tp, rake_fn, rake_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    rake_tp_i = set(rake_kw[i]).intersection(gold_keywords[i])\n",
    "    rake_tp.append(rake_tp_i)\n",
    "    rake_fn_i = gold_keywords[i].difference(set(rake_kw[i]))\n",
    "    rake_fn.append(rake_fn_i)\n",
    "    rake_fp_i = set(rake_kw[i]).difference(gold_keywords[i])\n",
    "    rake_fp.append(rake_fp_i)\n",
    "    print('keywords in common', rake_tp_i)\n",
    "    print('keywords not found', rake_fn_i)\n",
    "    print('extra keywords', rake_fp_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ccb62",
   "metadata": {},
   "source": [
    "#### 3. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee5bd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9492ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_kw_ = []\n",
    "for i in range(len(corpus)):\n",
    "    sum_kw_ += [keywords.keywords(normalized_texts[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63e52f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_kw = []\n",
    "for i in sum_kw_:\n",
    "    sum_kw += [i.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b45376ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common set()\n",
      "keywords not found {'vikram', 'skyroot', 'india', 'launch'}\n",
      "extra keywords {'technology', 'india private rocket', 'isro', 'richard', 'launched space indian startup skyroot aerospace launch country privately', 'year', 'test', 'accord', 'lift', 'hub'}\n",
      "TEXT 2\n",
      "keywords in common {'nba'}\n",
      "keywords not found {'twitter', 'doncic'}\n",
      "extra keywords {'game', 'star', 'way', 'week', 'month blue', 'chess twitter', 'account', 'follow', 'phone', 'win basketball', 'play', 'performance', 'verify', 'old', 'court build', 'musk', 'gaming company', 'question ask', 'decision', 'year', 'doncic say'}\n",
      "TEXT 3\n",
      "keywords in common set()\n",
      "keywords not found {'labor', 'strike', 'freight', 'union', 'industry'}\n",
      "extra keywords {'worker', 'union member avoid', 'sick pay', 'action congress block', 'say', 'freight railroads strike avoided', 'day', 'contract', 'peb', 'literally', 'labor deal reach', 'provide', 'proposal', 'railroad', 'far', 'request', 'ratification vote', 'clearly', 'clear', 'reject', 'represent nation', 'intervene', 'point', 'cnn', 'management industry', 'propose agreement', 'jefferies', 'come'}\n",
      "TEXT 4\n",
      "keywords in common {'agriculture', 'upside foods'}\n",
      "keywords not found {'food', 'fda'}\n",
      "extra keywords {'grow meat', 'evaluation', 'evaluate', 'base company', 'help', 'statement', 'fda say food', 'singapore', 'cultivate', 'process', 'approval', 'conclusion', 'valeti', 'sell product', 'cultured chicken cell', 'firm', 'animal', 'safety clearance'}\n",
      "TEXT 5\n",
      "keywords in common {'swift'}\n",
      "keywords not found {'eras tour', 'ticketmaster'}\n",
      "extra keywords {'website', 'say thursday', 'ticketmaster cancels', 'cancellation', 'sale', 'tour', 'verified fans', 'number fan', 'million', 'buy ticket', 'people', 'verify', 'platform', 'code', 'cancel', 'sell tuesday', 'demand', 'unprecedented'}\n",
      "TEXT 6\n",
      "keywords in common set()\n",
      "keywords not found {'mental health'}\n",
      "extra keywords {'provider', 'issue', 'kuhn', 'employer', 'past', 'related resource', 'grasso', 'say', 'offering', 'substance', 'access', 'long tail', 'george', 'provide', 'employee mental health', 'workforce', 'benefit', 'concern', 'limited deadly pandemic', 'relate', 'offer education', 'increase', 'disorder', 'mercer', 'educate'}\n",
      "TEXT 7\n",
      "keywords in common {'australian open', 'australia'}\n",
      "keywords not found {'tennis', 'grand slam', 'djokovic'}\n",
      "extra keywords {'deport', 'timing', 'january', 'course', 'tell', 'tiley', 'year', 'tuesday', 'minister', 'tennis player novak', 'time', 'obviously know', 'djokovic visa ban', 'opposition', 'risk', 'news press'}\n",
      "TEXT 8\n",
      "keywords in common {'oil', 'port'}\n",
      "keywords not found {'nigeria', 'africa', 'economy', 'lagos', 'lekki'}\n",
      "extra keywords {'container', 'close', 'important', 'project', 'development', 'billion lekki', 'contain', 'commercial', 'refinery', 'year', 'developer', 'country', 'hectare', 'african trade', 'accord', 'process import', 'complete nigeria'}\n",
      "TEXT 9\n",
      "keywords in common {'hunt', 'tax', 'economy', 'recession'}\n",
      "keywords not found {'government', 'united kingdom', 'jefferies'}\n",
      "extra keywords {'debt', 'bring', 'new', 'inflation', 'selfin', 'public', 'government united kingdom', 'household', 'obr', 'nearly', 'high taxes spending', 'taxis', 'budget plan', 'fall', 'say', 'britain', 'environment force', 'borrowing bid generate growth', 'accord', 'uk', 'expect', 'big', 'rock', 'rise', 'slightly', 'decision', 'year', 'finance solid', 'generator', 'minister', 'truss', 'people country', 'bank', 'investor'}\n",
      "TEXT 10\n",
      "keywords in common {'ronaldo', 'team', 'manchester united', 'season'}\n",
      "keywords not found {'champions league', 'ten hag', 'united', 'manchester'}\n",
      "extra keywords {'game', 'salary', 'manager', 'fail', 'today', 'cups', 'personal', 'superstar enjoy', 'good player world', 'hag', 'league', 'reason', 'force', 'elite', 'turn', 'medium', 'haul', 'late', 'feel', 'release week', 'summer', 'right', 'despite', 'quickly', 'return old', 'include', 'forward leave', 'villain', 'come', 'club', 'talent', 'end match', 'say', 'total', 'follow', 'focus', 'totally', 'hero', 'followed person', 'big', 'win', 'year', 'tv interview', 'baby', 'sign', 'bayern', 'quick', 'cup', 'believe'}\n"
     ]
    }
   ],
   "source": [
    "sum_tp, sum_fn, sum_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    sum_tp_i = set(sum_kw[i]).intersection(gold_keywords[i])\n",
    "    sum_tp.append(sum_tp_i)\n",
    "    sum_fn_i = gold_keywords[i].difference(set(sum_kw[i]))\n",
    "    sum_fn.append(sum_fn_i)\n",
    "    sum_fp_i = set(sum_kw[i]).difference(gold_keywords[i])\n",
    "    sum_fp.append(sum_fp_i)\n",
    "    print('keywords in common', sum_tp_i)\n",
    "    print('keywords not found', sum_fn_i)\n",
    "    print('extra keywords', sum_fp_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9998fc1",
   "metadata": {},
   "source": [
    "### 4. Составить морфологические/синтаксические шаблоны для ключевых слов и фраз, выделить соответствующие им подстроки из корпуса (например, именные группы Adj+Noun). Применить эти фильтры к спискам ключевых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c092a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'india', 'launch', 'skyroot', 'vikram'},\n",
       " {'doncic', 'nba', 'twitter'},\n",
       " {'freight', 'industry', 'labor', 'strike', 'union'},\n",
       " {'agriculture', 'fda', 'food', 'upside foods'},\n",
       " {'eras tour', 'swift', 'ticketmaster'},\n",
       " {'mental health'},\n",
       " {'australia', 'australian open', 'djokovic', 'grand slam', 'tennis'},\n",
       " {'africa', 'economy', 'lagos', 'lekki', 'nigeria', 'oil', 'port'},\n",
       " {'economy',\n",
       "  'government',\n",
       "  'hunt',\n",
       "  'jefferies',\n",
       "  'recession',\n",
       "  'tax',\n",
       "  'united kingdom'},\n",
       " {'champions league',\n",
       "  'manchester',\n",
       "  'manchester united',\n",
       "  'ronaldo',\n",
       "  'season',\n",
       "  'team',\n",
       "  'ten hag',\n",
       "  'united'}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007a039",
   "metadata": {},
   "source": [
    "Поскольку у меня среди ключевых слов почти исключительно существительные, сложно придумать, какие маски можно применить. Попробуем формулы (Noun), (Adj + Noun) и (Noun + Noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3c36b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1e64f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = [{'POS': {'IN': ['NOUN', 'PROPN']}}]\n",
    "adj_noun = [{'POS':'ADJ'}, {'POS':'NOUN'}]\n",
    "noun_noun = [{'POS':{'IN': ['NOUN', 'PROPN']}}, {'POS':{'IN': ['NOUN', 'PROPN']}}]\n",
    "matcher.add('n', [noun])\n",
    "matcher.add('an', [adj_noun], greedy='LONGEST')\n",
    "matcher.add('nn', [noun_noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b08731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c139c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:32<00:00,  9.29s/it]\n"
     ]
    }
   ],
   "source": [
    "matches = [[]]\n",
    "for i in tqdm(range(len(corpus))):\n",
    "    matches_ = matcher(nlp(corpus[i]))\n",
    "    for m, s, e in matches_:\n",
    "        matches[-1] += [nlp(corpus[i])[s:e]]\n",
    "    matches += [[]]\n",
    "    \n",
    "del matches[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "769d6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_texts = [' '.join([j.text for j in spacy.util.filter_spans(i)]) for i in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18ccd9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LAB MEAT HUMAN CONSUMPTION FDA US Food Drug Administration safety clearance lab meat first time Upside Foods California company meat cultured chicken cells products facilities US Department Agriculture agency information Upside Foods further questions time firm safety conclusion lab meat planet health FDA goal innovation food technologies first priority safety foods U.S. consumers statement Upside Foods founder CEO Uma Valeti Twitter chicken step tables UPSIDE Questions Letter FDA Valeti conclusion chicken CNN year process meat brewing beer yeast microbes animal cells products vegan vegetarian plant real meat animal Singapore first country sale cultured meat San Francisco Inc. regulatory approval laboratory chicken Singapore Advocates cultured meat need animals food climate crisis food system quarter global greenhouse gas emissions animal agriculture FDA historic announcement rigorous evaluation UPSIDE Foods first company world US FDA greenlight chicken David Kay director communications Upside Foods email scale meat less water land meat approval FDA market consultation process clearance food cultured chicken cells Upside statement FDA additional firms cultured animal cell food'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_texts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbbcd57",
   "metadata": {},
   "source": [
    "Посчитаем ключевые слова на новых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72ae1b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common {'vikram', 'skyroot', 'india', 'launch'}\n",
      "keywords not found set()\n",
      "extra keywords {'skyroot aerospace', 'indian', 'rocket', 'startup', 'isro', 'space', 'indian space', 'aerospace', 'private', 'launch skyroot', 'private rocket'}\n",
      "TEXT 2\n",
      "keywords in common {'twitter', 'doncic', 'nba'}\n",
      "keywords not found set()\n",
      "extra keywords {'luka doncic', 'basketball', 'chess', 'star', 'mavericks', 'musk', 'account', 'decision', 'luka', 'blue', 'accounts', 'tick'}\n",
      "TEXT 3\n",
      "keywords in common {'labor', 'strike', 'freight', 'union', 'industry'}\n",
      "keywords not found set()\n",
      "extra keywords {'railroads', 'sick', 'congress', 'unions', 'agreements', 'sick days', 'members', 'workers', 'deals', 'jefferies'}\n",
      "TEXT 4\n",
      "keywords in common {'food', 'fda', 'upside foods'}\n",
      "keywords not found {'agriculture'}\n",
      "extra keywords {'lab', 'foods', 'upside', 'us', 'first', 'lab meat', 'meat', 'cultured', 'safety', 'cells', 'chicken', 'animal'}\n",
      "TEXT 5\n",
      "keywords in common {'eras tour', 'ticketmaster', 'swift'}\n",
      "keywords not found set()\n",
      "extra keywords {'tickets', 'website', 'swift eras', 'outrage', 'tour', 'ticket', 'post', 'demand', 'fans', 'eras', 'tickets swift', 'unprecedented'}\n",
      "TEXT 6\n",
      "keywords in common {'mental health'}\n",
      "keywords not found set()\n",
      "extra keywords {'employee', 'workforce', 'health issues', 'health', 'kuhn', 'many', 'mental', 'employees', 'increase', 'employee mental', 'disorders', 'mercer', 'employers', 'issues'}\n",
      "TEXT 7\n",
      "keywords in common {'australian open', 'tennis', 'australia', 'djokovic'}\n",
      "keywords not found {'grand slam'}\n",
      "extra keywords {'visa', 'clarity', 'open', 'january', 'australian', 'news', 'tiley', 'grand', 'novak', 'ban', 'year ban'}\n",
      "TEXT 8\n",
      "keywords in common {'lekki', 'nigeria', 'lagos', 'port'}\n",
      "keywords not found {'oil', 'economy', 'africa'}\n",
      "extra keywords {'sea port', 'lekki deep', 'refinery', 'infrastructure projects', 'port refinery', 'hectare', 'sea', 'construction', 'trade', 'african', 'free'}\n",
      "TEXT 9\n",
      "keywords in common {'hunt', 'tax', 'recession', 'united kingdom', 'government'}\n",
      "keywords not found {'economy', 'jefferies'}\n",
      "extra keywords {'finances', 'borrowing', 'united', 'selfin', 'truss', 'public', 'kingdom', 'taxes', 'uk', 'obr'}\n",
      "TEXT 10\n",
      "keywords in common {'ronaldo', 'united', 'manchester', 'season', 'manchester united'}\n",
      "keywords not found {'team', 'ten hag', 'champions league'}\n",
      "extra keywords {'cristiano', 'club', 'respect', 'manager', 'ronaldo club', 'summer', 'cristiano ronaldo', 'hag', 'league', 'interview'}\n"
     ]
    }
   ],
   "source": [
    "# TD-IDF\n",
    "tfidf_ = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_m = tfidf_.fit_transform(masked_texts)\n",
    "\n",
    "feat_m = tfidf_.get_feature_names_out()\n",
    "tfidf_m_kw = []\n",
    "for i in range(len(corpus)):\n",
    "    doc = tfidf_m[i]\n",
    "    df = pd.DataFrame(doc.T.todense(), index=feat_m, columns=['weights'])\n",
    "    df = df.sort_values(by=['weights'], ascending=False)[:15]\n",
    "    tfidf_m_kw += [df.index.tolist()]\n",
    "    \n",
    "tfidf_m_tp, tfidf_m_fn, tfidf_m_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    tfidf_m_tp_i = set(tfidf_m_kw[i]).intersection(gold_keywords[i])\n",
    "    tfidf_m_tp.append(tfidf_m_tp_i)\n",
    "    tfidf_m_fn_i = gold_keywords[i].difference(set(tfidf_m_kw[i]))\n",
    "    tfidf_m_fn.append(tfidf_m_fn_i)\n",
    "    tfidf_m_fp_i = set(tfidf_m_kw[i]).difference(gold_keywords[i])\n",
    "    tfidf_m_fp.append(tfidf_m_fp_i)\n",
    "    print('keywords in common', tfidf_m_tp_i)\n",
    "    print('keywords not found', tfidf_m_fn_i)\n",
    "    print('extra keywords', tfidf_m_fp_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7dbaa911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common {'india'}\n",
      "keywords not found {'vikram', 'skyroot', 'launch'}\n",
      "extra keywords {'year', 's'}\n",
      "TEXT 2\n",
      "keywords in common {'nba'}\n",
      "keywords not found {'twitter', 'doncic'}\n",
      "extra keywords set()\n",
      "TEXT 3\n",
      "keywords in common set()\n",
      "keywords not found {'labor', 'strike', 'freight', 'union', 'industry'}\n",
      "extra keywords {'freight railroads'}\n",
      "TEXT 4\n",
      "keywords in common set()\n",
      "keywords not found {'agriculture', 'food', 'fda', 'upside foods'}\n",
      "extra keywords {'s', 'company world'}\n",
      "TEXT 5\n",
      "keywords in common set()\n",
      "keywords not found {'eras tour', 'ticketmaster', 'swift'}\n",
      "extra keywords set()\n",
      "TEXT 6\n",
      "keywords in common set()\n",
      "keywords not found {'mental health'}\n",
      "extra keywords set()\n",
      "TEXT 7\n",
      "keywords in common set()\n",
      "keywords not found {'australian open', 'grand slam', 'australia', 'djokovic', 'tennis'}\n",
      "extra keywords {'world'}\n",
      "TEXT 8\n",
      "keywords in common {'nigeria'}\n",
      "keywords not found {'africa', 'oil', 'economy', 'lagos', 'port', 'lekki'}\n",
      "extra keywords {'construction'}\n",
      "TEXT 9\n",
      "keywords in common set()\n",
      "keywords not found {'united kingdom', 'economy', 'government', 'hunt', 'tax', 'jefferies', 'recession'}\n",
      "extra keywords set()\n",
      "TEXT 10\n",
      "keywords in common set()\n",
      "keywords not found {'ronaldo', 'team', 'champions league', 'ten hag', 'united', 'manchester', 'season', 'manchester united'}\n",
      "extra keywords {'players', 'success', 'stint club', 'wonder', 'clubs forward'}\n"
     ]
    }
   ],
   "source": [
    "# RAKE\n",
    "rake_m_kw_ = []\n",
    "for i in range(len(corpus)):\n",
    "    rake_m_kw_ += [rake.run(masked_texts[i], maxWords = 2, minFrequency=1)]\n",
    "    \n",
    "rake_m_kw = []\n",
    "for rkw in rake_m_kw_:\n",
    "    if rkw:\n",
    "        rake_m_kw += [[i[0] for i in rkw]]\n",
    "    else:\n",
    "        rake_m_kw += [[]]\n",
    "\n",
    "\n",
    "rake_m_tp, rake_m_fn, rake_m_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    rake_m_tp_i = set(rake_m_kw[i]).intersection(gold_keywords[i])\n",
    "    rake_m_tp.append(rake_m_tp_i)\n",
    "    rake_m_fn_i = gold_keywords[i].difference(set(rake_m_kw[i]))\n",
    "    rake_m_fn.append(rake_m_fn_i)\n",
    "    rake_m_fp_i = set(rake_m_kw[i]).difference(gold_keywords[i])\n",
    "    rake_m_fp.append(rake_m_fp_i)\n",
    "    print('keywords in common', rake_m_tp_i)\n",
    "    print('keywords not found', rake_m_fn_i)\n",
    "    print('extra keywords', rake_m_fp_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63715ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1\n",
      "keywords in common {'skyroot', 'india', 'launch'}\n",
      "keywords not found {'vikram'}\n",
      "extra keywords {'indian', 'rocket', 'technologies', 'isro', 'space', 'aerospace country', 'private', 'launched', 'firm', 'jeff'}\n",
      "TEXT 2\n",
      "keywords in common {'twitter', 'doncic', 'nba'}\n",
      "keywords not found set()\n",
      "extra keywords {'chess', 'star', 'decision month blue', 'account', 'phone', 'time', 'question', 'basketball', 'court', 'game performance', 'questions', 'musk', 'gaming company', 'year', 'accounts'}\n",
      "TEXT 3\n",
      "keywords in common {'freight', 'strike', 'industry'}\n",
      "keywords not found {'union', 'labor'}\n",
      "extra keywords {'contracts railroad', 'deal agreement', 'nations', 'railroads', 'way', 'sick', 'union members', 'recommendation', 'peb', 'labor deals', 'recommendations member', 'action', 'employee', 'ratification', 'nation', 'congress', 'unions agreements', 'sides', 'workers', 'employees', 'best', 'jefferies', 'management', 'cnn'}\n",
      "TEXT 4\n",
      "keywords in common {'agriculture', 'food', 'fda'}\n",
      "keywords not found {'upside foods'}\n",
      "extra keywords {'foods', 'cultured chicken cells products', 'approval', 'process', 'upside', 'conclusion', 'animals', 'firms', 'cell', 'valeti', 'meat', 'safety clearance', 'firm', 'animal'}\n",
      "TEXT 5\n",
      "keywords in common {'ticketmaster', 'swift'}\n",
      "keywords not found {'eras tour'}\n",
      "extra keywords {'tickets', 'website', 'codes', 'fan', 'demands', 'number fans', 'sale', 'tour', 'company', 'bots', 'thursday', 'demand ticket', 'platform', 'bot', 'sales', 'unprecedented'}\n",
      "TEXT 6\n",
      "keywords in common set()\n",
      "keywords not found {'mental health'}\n",
      "extra keywords {'deadly pandemic', 'kuhn', 'grasso', 'substance', 'resources', 'benefits', 'access', 'concerns', 'george', 'floyd', 'employee', 'workforce', 'concern', 'health', 'way stress drug consumption', 'mental', 'employees', 'increase', 'mercer', 'employers', 'issues'}\n",
      "TEXT 7\n",
      "keywords in common {'australia', 'tennis', 'djokovic'}\n",
      "keywords not found {'australian open', 'grand slam'}\n",
      "extra keywords {'visa', 'timing', 'open', 'tiley', 'australian', 'years', 'year', 'novak', 'tuesday', 'minister', 'ban', 'time', 'country', 'news press'}\n",
      "TEXT 8\n",
      "keywords in common {'nigeria', 'oil', 'lagos', 'port', 'lekki'}\n",
      "keywords not found {'economy', 'africa'}\n",
      "extra keywords {'container', 'development', 'developers years', 'refinery', 'year', 'country', 'hectare', 'african trade', 'area', 'areas'}\n",
      "TEXT 9\n",
      "keywords in common {'tax', 'economy'}\n",
      "keywords not found {'hunt', 'recession', 'united kingdom', 'government', 'jefferies'}\n",
      "extra keywords {'debt', 'countries', 'new', 'households', 'kingdom recession', 'debts', 'selfin', 'public', 'hunt budget plan', 'household', 'obr', 'level', 'britons', 'government finances', 'taxes', 'years', 'banks', 'spending', 'uk', 'levels', 'environment', 'borrowing', 'big', 'united', 'investors', 'year', 'minister', 'governments', 'people country', 'plans', 'bank', 'truss', 'finance'}\n",
      "TEXT 10\n",
      "keywords in common {'ronaldo', 'team', 'manchester', 'season'}\n",
      "keywords not found {'champions league', 'ten hag', 'united', 'manchester united'}\n",
      "extra keywords {'cristiano', 'game', 'manager', 'tv', 'tucker', 'cups', 'personal', 'teams', 'reason', 'league', 'thing', 'time', 'elite', 'clubs', 'cup', 'haul', 'united hero villain', 'glazer', 'interview', 'club', 'players', 'summer years', 'end match', 'week', 'focus', 'glazers', 'statement', 'player world', 'superstar', 'person', 'media', 'year', 'bayern', 'baby', 'matches', 'today interviews', 'reasons', 'ferdinand'}\n"
     ]
    }
   ],
   "source": [
    "# TextRank\n",
    "\n",
    "sum_m_kw_ = []\n",
    "for i in range(len(corpus)):\n",
    "    sum_m_kw_ += [keywords.keywords(masked_texts[i])]\n",
    "    \n",
    "sum_m_kw = []\n",
    "for i in sum_m_kw_:\n",
    "    sum_m_kw += [i.split('\\n')]\n",
    "\n",
    "sum_m_tp, sum_m_fn, sum_m_fp = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    print(f'TEXT {i+1}')\n",
    "    sum_m_tp_i = set(sum_m_kw[i]).intersection(gold_keywords[i])\n",
    "    sum_m_tp.append(sum_m_tp_i)\n",
    "    sum_m_fn_i = gold_keywords[i].difference(set(sum_m_kw[i]))\n",
    "    sum_m_fn.append(sum_m_fn_i)\n",
    "    sum_m_fp_i = set(sum_m_kw[i]).difference(gold_keywords[i])\n",
    "    sum_m_fp.append(sum_m_fp_i)\n",
    "    print('keywords in common', sum_m_tp_i)\n",
    "    print('keywords not found', sum_m_fn_i)\n",
    "    print('extra keywords', sum_m_fp_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7096a",
   "metadata": {},
   "source": [
    "### 5. Оценить точность, полноту, F-меру выбранных методов относительно эталона: с учётом морфосинтаксических шаблонов и без них."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c62c94f",
   "metadata": {},
   "source": [
    "Можно считать для каждого текста каждое значение по отдельности, а потом брать среднее для метода значение по всем текстам. А можно посчитать суммы всех текстов в корпусе (сколько всего слов угадал, сколько всего лишних и т.д.), и потом считать метрику по этим суммам. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b066187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b89bd718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean TF-IDF precision: 0.325,\n",
      "Mean TF-IDF recall: 0.6166666666666667,\n",
      "Mean TF-IDF F1-score: 0.40005439005439003\n",
      "\n",
      "\n",
      "Total TF-IDF precision: 0.325,\n",
      "Total TF-IDF recall: 0.5531914893617021,\n",
      "Total TF-IDF F1-score: 0.4094488188976378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean TF-IDF across all texts\n",
    "\n",
    "tfidf_precision, tfidf_recall, tfidf_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    tfidf_precision.append(len(tfidf_tp[i]) / (len(tfidf_tp[i]) + len(tfidf_fp[i])))\n",
    "    tfidf_recall.append(len(tfidf_tp[i]) / (len(tfidf_tp[i]) + len(tfidf_fn[i])))\n",
    "    tfidf_f1.append(2 * (tfidf_precision[i] * tfidf_recall[i]) / (tfidf_precision[i] + tfidf_recall[i]))\n",
    "    \n",
    "print(f'''\n",
    "Mean TF-IDF precision: {mean(tfidf_precision)},\n",
    "Mean TF-IDF recall: {mean(tfidf_recall)},\n",
    "Mean TF-IDF F1-score: {mean(tfidf_f1)}\n",
    "''')\n",
    "\n",
    "# total TF-IDF as if all texts were one \n",
    "\n",
    "tfidf_precision = len([j for i in tfidf_tp for j in i]) / (len([j for i in tfidf_tp for j in i]) + len([j for i in tfidf_fp for j in i]))\n",
    "tfidf_recall = len([j for i in tfidf_tp for j in i]) / (len([j for i in tfidf_tp for j in i]) + len([j for i in tfidf_fn for j in i]))\n",
    "tfidf_f1 = 2 * (tfidf_precision * tfidf_recall) / (tfidf_precision + tfidf_recall)\n",
    "print(f'''\n",
    "Total TF-IDF precision: {tfidf_precision},\n",
    "Total TF-IDF recall: {tfidf_recall},\n",
    "Total TF-IDF F1-score: {tfidf_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b87ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean RAKE precision: 0.245,\n",
      "Mean RAKE recall: 0.10511904761904761,\n",
      "Mean RAKE F1-score: 0.14547119547119547\n",
      "\n",
      "\n",
      "Total RAKE precision: 0.29411764705882354,\n",
      "Total RAKE recall: 0.10638297872340426,\n",
      "Total RAKE F1-score: 0.15625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean RAKE across all texts\n",
    "\n",
    "rake_precision, rake_recall, rake_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    rake_precision.append(len(rake_tp[i]) / max(0.01, (len(rake_tp[i]) + len(rake_fp[i]))))\n",
    "    rake_recall.append(len(rake_tp[i]) / max(0.01, (len(rake_tp[i]) + len(rake_fn[i]))))\n",
    "    rake_f1.append(2 * (rake_precision[i] * rake_recall[i]) / max(0.01, (rake_precision[i] + rake_recall[i])))\n",
    "    \n",
    "print(f'''\n",
    "Mean RAKE precision: {mean(rake_precision)},\n",
    "Mean RAKE recall: {mean(rake_recall)},\n",
    "Mean RAKE F1-score: {mean(rake_f1)}\n",
    "''')\n",
    "\n",
    "# total RAKE as if all texts were one \n",
    "\n",
    "rake_precision = len([j for i in rake_tp for j in i]) / (len([j for i in rake_tp for j in i]) + len([j for i in rake_fp for j in i]))\n",
    "rake_recall = len([j for i in rake_tp for j in i]) / (len([j for i in rake_tp for j in i]) + len([j for i in rake_fn for j in i]))\n",
    "rake_f1 = 2 * (rake_precision * rake_recall) / max(0.01, (rake_precision + rake_recall))\n",
    "print(f'''\n",
    "Total RAKE precision: {rake_precision},\n",
    "Total RAKE recall: {rake_recall},\n",
    "Total RAKE F1-score: {rake_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b627cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean TextRank precision: 0.05951952494157062,\n",
      "Mean TextRank recall: 0.29238095238095235,\n",
      "Mean TextRank F1-score: 0.09742602736615567\n",
      "\n",
      "\n",
      "Total TextRank precision: 0.06349206349206349,\n",
      "Total TextRank recall: 0.3404255319148936,\n",
      "Total TextRank F1-score: 0.10702341137123746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean TextRank across all texts\n",
    "\n",
    "sum_precision, sum_recall, sum_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    sum_precision.append(len(sum_tp[i]) / max(0.01, (len(sum_tp[i]) + len(sum_fp[i]))))\n",
    "    sum_recall.append(len(sum_tp[i]) / max(0.01, (len(sum_tp[i]) + len(sum_fn[i]))))\n",
    "    sum_f1.append(2 * (sum_precision[i] * sum_recall[i]) / max(0.01, (sum_precision[i] + sum_recall[i])))\n",
    "    \n",
    "print(f'''\n",
    "Mean TextRank precision: {mean(sum_precision)},\n",
    "Mean TextRank recall: {mean(sum_recall)},\n",
    "Mean TextRank F1-score: {mean(sum_f1)}\n",
    "''')\n",
    "\n",
    "# total TextRank as if all texts were one \n",
    "\n",
    "sum_precision = len([j for i in sum_tp for j in i]) / (len([j for i in sum_tp for j in i]) + len([j for i in sum_fp for j in i]))\n",
    "sum_recall = len([j for i in sum_tp for j in i]) / (len([j for i in sum_tp for j in i]) + len([j for i in sum_fn for j in i]))\n",
    "sum_f1 = 2 * (sum_precision * sum_recall) / max(0.01, (sum_precision + sum_recall))\n",
    "print(f'''\n",
    "Total TextRank precision: {sum_precision},\n",
    "Total TextRank recall: {sum_recall},\n",
    "Total TextRank F1-score: {sum_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11342cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean masked TF-IDF precision: 0.24666666666666667,\n",
      "Mean masked TF-IDF recall: 0.8460714285714286,\n",
      "Mean masked TF-IDF F1-score: 0.3681473198807295\n",
      "\n",
      "\n",
      "Total masked TF-IDF precision: 0.24666666666666667,\n",
      "Total masked TF-IDF recall: 0.7872340425531915,\n",
      "Total masked TF-IDF F1-score: 0.3756345177664975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean masked TF-IDF across all texts\n",
    "\n",
    "tfidf_m_precision, tfidf_m_recall, tfidf_m_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    tfidf_m_precision.append(len(tfidf_m_tp[i]) / (len(tfidf_m_tp[i]) + len(tfidf_m_fp[i])))\n",
    "    tfidf_m_recall.append(len(tfidf_m_tp[i]) / (len(tfidf_m_tp[i]) + len(tfidf_m_fn[i])))\n",
    "    tfidf_m_f1.append(2 * (tfidf_m_precision[i] * tfidf_m_recall[i]) / (tfidf_m_precision[i] + tfidf_m_recall[i]))\n",
    "    \n",
    "print(f'''\n",
    "Mean masked TF-IDF precision: {mean(tfidf_m_precision)},\n",
    "Mean masked TF-IDF recall: {mean(tfidf_m_recall)},\n",
    "Mean masked TF-IDF F1-score: {mean(tfidf_m_f1)}\n",
    "''')\n",
    "\n",
    "# total masked TF-IDF as if all texts were one \n",
    "\n",
    "tfidf_m_precision = len([j for i in tfidf_m_tp for j in i]) / (len([j for i in tfidf_m_tp for j in i]) + len([j for i in tfidf_m_fp for j in i]))\n",
    "tfidf_m_recall = len([j for i in tfidf_m_tp for j in i]) / (len([j for i in tfidf_m_tp for j in i]) + len([j for i in tfidf_m_fn for j in i]))\n",
    "tfidf_m_f1 = 2 * (tfidf_m_precision * tfidf_m_recall) / (tfidf_m_precision + tfidf_m_recall)\n",
    "print(f'''\n",
    "Total masked TF-IDF precision: {tfidf_m_precision},\n",
    "Total masked TF-IDF recall: {tfidf_m_recall},\n",
    "Total masked TF-IDF F1-score: {tfidf_m_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5422e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean masked RAKE precision: 0.18333333333333332,\n",
      "Mean masked RAKE recall: 0.07261904761904761,\n",
      "Mean masked RAKE F1-score: 0.1007936507936508\n",
      "\n",
      "\n",
      "Total masked RAKE precision: 0.2,\n",
      "Total masked RAKE recall: 0.06382978723404255,\n",
      "Total masked RAKE F1-score: 0.0967741935483871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean masked RAKE across all texts\n",
    "\n",
    "rake_m_precision, rake_m_recall, rake_m_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    rake_m_precision.append(len(rake_m_tp[i]) / max(0.01, (len(rake_m_tp[i]) + len(rake_m_fp[i]))))\n",
    "    rake_m_recall.append(len(rake_m_tp[i]) / (len(rake_m_tp[i]) + len(rake_m_fn[i])))\n",
    "    rake_m_f1.append(2 * (rake_m_precision[i] * rake_m_recall[i]) / max(0.01, (rake_m_precision[i] + rake_m_recall[i])))\n",
    "    \n",
    "print(f'''\n",
    "Mean masked RAKE precision: {mean(rake_m_precision)},\n",
    "Mean masked RAKE recall: {mean(rake_m_recall)},\n",
    "Mean masked RAKE F1-score: {mean(rake_m_f1)}\n",
    "''')\n",
    "\n",
    "# total masked RAKE as if all texts were one \n",
    "\n",
    "rake_m_precision = len([j for i in rake_m_tp for j in i]) / max(0.01, (len([j for i in rake_m_tp for j in i]) + len([j for i in rake_m_fp for j in i])))\n",
    "rake_m_recall = len([j for i in rake_m_tp for j in i]) / (len([j for i in rake_m_tp for j in i]) + len([j for i in rake_m_fn for j in i]))\n",
    "rake_m_f1 = 2 * (rake_m_precision * rake_m_recall) / max(0.01, (rake_m_precision + rake_m_recall))\n",
    "print(f'''\n",
    "Total masked RAKE precision: {rake_m_precision},\n",
    "Total masked RAKE recall: {rake_m_recall},\n",
    "Total masked RAKE F1-score: {rake_m_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e331f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean masked TextRank precision: 0.14583135818429935,\n",
      "Mean masked TextRank recall: 0.5866666666666667,\n",
      "Mean masked TextRank F1-score: 0.22848567608861725\n",
      "\n",
      "\n",
      "Total masked TextRank precision: 0.12556053811659193,\n",
      "Total masked TextRank recall: 0.5957446808510638,\n",
      "Total masked TextRank F1-score: 0.2074074074074074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean masked TextRank across all texts\n",
    "\n",
    "sum_m_precision, sum_m_recall, sum_m_f1 = [], [], []\n",
    "for i in range(len(corpus)):\n",
    "    sum_m_precision.append(len(sum_m_tp[i]) / max(0.01, (len(sum_m_tp[i]) + len(sum_m_fp[i]))))\n",
    "    sum_m_recall.append(len(sum_m_tp[i]) / (len(sum_m_tp[i]) + len(sum_m_fn[i])))\n",
    "    sum_m_f1.append(2 * (sum_m_precision[i] * sum_m_recall[i]) / max(0.01, (sum_m_precision[i] + sum_m_recall[i])))\n",
    "    \n",
    "print(f'''\n",
    "Mean masked TextRank precision: {mean(sum_m_precision)},\n",
    "Mean masked TextRank recall: {mean(sum_m_recall)},\n",
    "Mean masked TextRank F1-score: {mean(sum_m_f1)}\n",
    "''')\n",
    "\n",
    "# total masked TextRank as if all texts were one \n",
    "\n",
    "sum_m_precision = len([j for i in sum_m_tp for j in i]) / max(0.01, (len([j for i in sum_m_tp for j in i]) + len([j for i in sum_m_fp for j in i])))\n",
    "sum_m_recall = len([j for i in sum_m_tp for j in i]) / (len([j for i in sum_m_tp for j in i]) + len([j for i in sum_m_fn for j in i]))\n",
    "sum_m_f1 = 2 * (sum_m_precision * sum_m_recall) / max(0.01, (sum_m_precision + sum_m_recall))\n",
    "print(f'''\n",
    "Total masked TextRank precision: {sum_m_precision},\n",
    "Total masked TextRank recall: {sum_m_recall},\n",
    "Total masked TextRank F1-score: {sum_m_f1}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caa0dc",
   "metadata": {},
   "source": [
    "### 6. Описать ошибки автоматического выделения ключевых слов (что выделяется лишнее, что не выделяется); предложить свои методы решения этих проблем."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29854bc",
   "metadata": {},
   "source": [
    "Мне кажется, низкий результат особенно у RAKE и TextRank в первую очередь объясняется длиной текстов. Если TF-IDF берёт внимание весь корпус и получает благодаря этому F1 score 0.5--0.6, то другие два алгоритма учатся только на одном тексте; а самые короткие тексты в этом корпусе длиной 200-300 слов, и алгоритмам недостаточно данных для качественной работы. Иногда они выделяют слишком много ключевых слов, и из-за этого падает precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887120a",
   "metadata": {},
   "source": [
    "Можно было бы ограничивать количество выбираемых ключевых слов искуственно, подгоняя его под количество слов в эталоне для этого текста. Это могло бы улучшить результат TextRank, который выделяет много ключевых слов; однако это не получится сделать для RAKE, который даже так в некоторых текстах не смог выделить ключевые слова для порога minFrequency=2. Снижать этот порог кажется неразумным, потому что все ключевые слова (в том числе эталонные) встречаются в тексте не менее 2-х раз. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067a237",
   "metadata": {},
   "source": [
    "По итогу, мне кажется, что TF-IDF показал лучший результат, потому что к такому формату коррпуса он подходит лучше всего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5c54c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
